{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd057f55249afac9e3bb90b27c0916a1d44f0a08c86299e4ac4c83ac98b0a805cf4",
   "display_name": "Python 3.8.8 64-bit ('pysyft': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "57f55249afac9e3bb90b27c0916a1d44f0a08c86299e4ac4c83ac98b0a805cf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# å®‰å…¨è®­ç»ƒè¯„ä¼°\n",
    "\n",
    "åœ¨æž„å»ºæœºå™¨å­¦ä¹ å³æœåŠ¡è§£å†³æ–¹æ¡ˆï¼ˆMLaaSï¼‰æ—¶ï¼Œå…¬å¸å¯èƒ½éœ€è¦è¯·æ±‚å…¶ä»–åˆä½œä¼™ä¼´è®¿é—®æ•°æ®ä»¥è®­ç»ƒå…¶æ¨¡åž‹ã€‚åœ¨å«ç”Ÿæˆ–é‡‘èžé¢†åŸŸï¼Œæ¨¡åž‹å’Œæ•°æ®éƒ½éžå¸¸å…³é”®ï¼šæ¨¡åž‹å‚æ•°æ˜¯ä¸šåŠ¡èµ„äº§ï¼Œè€Œæ•°æ®æ˜¯ä¸¥æ ¼ç›‘ç®¡çš„ä¸ªäººæ•°æ®ã€‚\n",
    "\n",
    "åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸€ç§å¯èƒ½çš„è§£å†³æ–¹æ¡ˆæ˜¯å¯¹æ¨¡åž‹å’Œæ•°æ®éƒ½è¿›è¡ŒåŠ å¯†ï¼Œå¹¶åœ¨åŠ å¯†åŽçš„å€¼ä¸Šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡åž‹ã€‚ä¾‹å¦‚ï¼Œè¿™ä¿è¯äº†å…¬å¸ä¸ä¼šè®¿é—®æ‚£è€…çš„ç—…åŽ†ï¼Œå¹¶ä¸”åŒ»ç–—æœºæž„å°†æ— æ³•è§‚å¯Ÿä»–ä»¬æ‰€è´¡çŒ®çš„æ¨¡åž‹ã€‚å­˜åœ¨å‡ ç§å…è®¸å¯¹åŠ å¯†æ•°æ®è¿›è¡Œè®¡ç®—çš„åŠ å¯†æ–¹æ¡ˆï¼Œå…¶ä¸­åŒ…æ‹¬å®‰å…¨å¤šæ–¹è®¡ç®—ï¼ˆSMPCï¼‰ï¼ŒåŒæ€åŠ å¯†ï¼ˆFHE / SHEï¼‰å’ŒåŠŸèƒ½åŠ å¯†ï¼ˆFEï¼‰ã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡Œé›†ä¸­è®¨è®ºå¤šæ–¹è®¡ç®—ï¼ˆå·²åœ¨æ•™ç¨‹5ä¸­è¿›è¡Œäº†ä»‹ç»ï¼‰ï¼Œå®ƒç”±ç§æœ‰åŠ æ€§å…±äº«ç»„æˆï¼Œå¹¶ä¾èµ–äºŽåŠ å¯†åè®®SecureNNå’ŒSPDZã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹çš„ç¡®åˆ‡è®¾ç½®å¦‚ä¸‹ï¼šè€ƒè™‘æ‚¨æ˜¯æœåŠ¡å™¨ï¼Œå¹¶ä¸”æ‚¨æƒ³å¯¹æ¨¡åž‹ä¸­çš„æŸäº›æ•°æ®è¿›è¡Œè®­ç»ƒã€‚  ð‘› å·¥äººã€‚æœåŠ¡å™¨æœºå¯†å…±äº«ä»–çš„æ¨¡åž‹ï¼Œå¹¶å°†æ¯ä¸ªå…±äº«å‘é€ç»™å·¥ä½œäººå‘˜ã€‚å·¥äººä»¬è¿˜ç§˜å¯†å…±äº«ä»–ä»¬çš„æ•°æ®å¹¶åœ¨ä»–ä»¬ä¹‹é—´äº¤æ¢æ•°æ®ã€‚åœ¨æˆ‘ä»¬å°†è¦ç ”ç©¶çš„é…ç½®ä¸­ï¼Œæœ‰2ä¸ªå·¥äººï¼šaliceå’Œbobã€‚äº¤æ¢è‚¡ä»½åŽï¼Œä»–ä»¬æ¯ä¸ªäººçŽ°åœ¨æ‹¥æœ‰è‡ªå·±çš„è‚¡ä»½ï¼Œå¦ä¸€å·¥äººçš„è‚¡ä»½å’Œæ¨¡åž‹çš„è‚¡ä»½ã€‚çŽ°åœ¨ï¼Œè®¡ç®—å¯ä»¥å¼€å§‹ä½¿ç”¨é€‚å½“çš„åŠ å¯†åè®®å¯¹æ¨¡åž‹è¿›è¡Œç§ä¸‹è®­ç»ƒã€‚è®­ç»ƒæ¨¡åž‹åŽï¼Œæ‰€æœ‰ä»½é¢éƒ½å¯ä»¥å‘é€å›žæœåŠ¡å™¨ä»¥å¯¹å…¶è¿›è¡Œè§£å¯†ã€‚ä¸‹å›¾å¯¹æ­¤è¿›è¡Œäº†è¯´æ˜Žï¼š"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 640\n",
    "n_test_items = 640"
   ]
  },
  {
   "source": [
    "## 1 å¯¼å…¥ä¸Žé…ç½®"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.02\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch to add extra functionalities like Federated and Encrypted Learning\n",
    "\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "source": [
    "## 2 ç§˜å¯†å…±äº«æ•°æ®\n",
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªæ•ˆç”¨å‡½æ•°æ¥æ¨¡æ‹Ÿä»¥ä¸‹è¡Œä¸ºï¼šæˆ‘ä»¬å‡è®¾MNISTæ•°æ®é›†åˆ†å¸ƒåœ¨å„ä¸ªéƒ¨åˆ†ä¸­ï¼Œæ¯ä¸ªéƒ¨åˆ†éƒ½ç”±æˆ‘ä»¬çš„ä¸€ä¸ªå·¥äººæŒæœ‰ã€‚ç„¶åŽï¼Œå·¥ä½œäººå‘˜å°†å…¶æ•°æ®åˆ†æ‰¹æ‹†åˆ†ï¼Œå¹¶åœ¨å½¼æ­¤ä¹‹é—´ç§˜å¯†å…±äº«å…¶æ•°æ®ã€‚è¿”å›žçš„æœ€ç»ˆå¯¹è±¡æ˜¯è¿™äº›ç§˜å¯†å…±äº«æ‰¹æ¬¡ä¸Šçš„å¯è¿­ä»£å¯¹è±¡ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºç§æœ‰æ•°æ®åŠ è½½å™¨ã€‚è¯·æ³¨æ„ï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæœ¬åœ°å·¥ä½œäººå‘˜ï¼ˆå› æ­¤æˆ‘ä»¬ï¼‰ä»Žæœªè®¿é—®è¿‡æ•°æ®ã€‚\n",
    "\n",
    "æˆ‘ä»¬åƒå¾€å¸¸ä¸€æ ·èŽ·å¾—äº†è®­ç»ƒå’Œæµ‹è¯•ç§æœ‰æ•°æ®é›†ï¼Œå¹¶ä¸”è¾“å…¥å’Œæ ‡ç­¾éƒ½æ˜¯ç§˜å¯†å…±äº«çš„ã€‚"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):\n",
    "        \"\"\"\n",
    "        Transform to one hot tensor\n",
    "        \n",
    "        Example:\n",
    "            [0, 3, 9]\n",
    "            =>\n",
    "            [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "             [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]]\n",
    "            \n",
    "        \"\"\"\n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 10) # 10 classes for MNIST\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor):\n",
    "        \"\"\"\n",
    "        Transform to fixed precision and secret share a tensor\n",
    "        \"\"\"\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True, transform=transformation),\n",
    "        batch_size=args.batch_size\n",
    "    )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, download=True, transform=transformation),\n",
    "        batch_size=args.test_batch_size\n",
    "    )\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "source": [
    "## 3 å®žçŽ°æ¨¡åž‹\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "## 4 è®­ç»ƒå’Œæµ‹è¯•"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "source": [
    "## 5 è®­ç»ƒ"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train Epoch: 1 [0/640 (0%)]\tLoss: 1.128000\tTime: 3.911s\n",
      "Train Epoch: 1 [64/640 (10%)]\tLoss: 1.012000\tTime: 3.977s\n",
      "Train Epoch: 1 [128/640 (20%)]\tLoss: 0.989000\tTime: 4.051s\n",
      "Train Epoch: 1 [192/640 (30%)]\tLoss: 0.902000\tTime: 4.048s\n",
      "Train Epoch: 1 [256/640 (40%)]\tLoss: 0.888000\tTime: 3.872s\n",
      "Train Epoch: 1 [320/640 (50%)]\tLoss: 0.876000\tTime: 3.966s\n",
      "Train Epoch: 1 [384/640 (60%)]\tLoss: 0.854000\tTime: 3.994s\n",
      "Train Epoch: 1 [448/640 (70%)]\tLoss: 0.853000\tTime: 4.016s\n",
      "Train Epoch: 1 [512/640 (80%)]\tLoss: 0.829000\tTime: 4.067s\n",
      "Train Epoch: 1 [576/640 (90%)]\tLoss: 0.841000\tTime: 4.133s\n",
      "\n",
      "Test set: Accuracy: 227.0/640 (35%)\n",
      "\n",
      "Train Epoch: 2 [0/640 (0%)]\tLoss: 0.781000\tTime: 3.979s\n",
      "Train Epoch: 2 [64/640 (10%)]\tLoss: 0.733000\tTime: 3.990s\n",
      "Train Epoch: 2 [128/640 (20%)]\tLoss: 0.791000\tTime: 4.032s\n",
      "Train Epoch: 2 [192/640 (30%)]\tLoss: 0.717000\tTime: 4.037s\n",
      "Train Epoch: 2 [256/640 (40%)]\tLoss: 0.707000\tTime: 4.151s\n",
      "Train Epoch: 2 [320/640 (50%)]\tLoss: 0.706000\tTime: 3.998s\n",
      "Train Epoch: 2 [384/640 (60%)]\tLoss: 0.709000\tTime: 4.048s\n",
      "Train Epoch: 2 [448/640 (70%)]\tLoss: 0.721000\tTime: 4.199s\n",
      "Train Epoch: 2 [512/640 (80%)]\tLoss: 0.710000\tTime: 4.113s\n",
      "Train Epoch: 2 [576/640 (90%)]\tLoss: 0.743000\tTime: 4.070s\n",
      "\n",
      "Test set: Accuracy: 360.0/640 (56%)\n",
      "\n",
      "Train Epoch: 3 [0/640 (0%)]\tLoss: 0.667000\tTime: 3.908s\n",
      "Train Epoch: 3 [64/640 (10%)]\tLoss: 0.596000\tTime: 3.894s\n",
      "Train Epoch: 3 [128/640 (20%)]\tLoss: 0.692000\tTime: 3.958s\n",
      "Train Epoch: 3 [192/640 (30%)]\tLoss: 0.600000\tTime: 3.913s\n",
      "Train Epoch: 3 [256/640 (40%)]\tLoss: 0.589000\tTime: 3.933s\n",
      "Train Epoch: 3 [320/640 (50%)]\tLoss: 0.590000\tTime: 3.900s\n",
      "Train Epoch: 3 [384/640 (60%)]\tLoss: 0.606000\tTime: 3.946s\n",
      "Train Epoch: 3 [448/640 (70%)]\tLoss: 0.628000\tTime: 4.041s\n",
      "Train Epoch: 3 [512/640 (80%)]\tLoss: 0.619000\tTime: 3.918s\n",
      "Train Epoch: 3 [576/640 (90%)]\tLoss: 0.668000\tTime: 3.974s\n",
      "\n",
      "Test set: Accuracy: 401.0/640 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/640 (0%)]\tLoss: 0.584000\tTime: 3.958s\n",
      "Train Epoch: 4 [64/640 (10%)]\tLoss: 0.499000\tTime: 4.126s\n",
      "Train Epoch: 4 [128/640 (20%)]\tLoss: 0.618000\tTime: 3.908s\n",
      "Train Epoch: 4 [192/640 (30%)]\tLoss: 0.518000\tTime: 3.876s\n",
      "Train Epoch: 4 [256/640 (40%)]\tLoss: 0.512000\tTime: 3.933s\n",
      "Train Epoch: 4 [320/640 (50%)]\tLoss: 0.511000\tTime: 3.976s\n",
      "Train Epoch: 4 [384/640 (60%)]\tLoss: 0.535000\tTime: 3.972s\n",
      "Train Epoch: 4 [448/640 (70%)]\tLoss: 0.562000\tTime: 3.889s\n",
      "Train Epoch: 4 [512/640 (80%)]\tLoss: 0.552000\tTime: 3.968s\n",
      "Train Epoch: 4 [576/640 (90%)]\tLoss: 0.611000\tTime: 4.015s\n",
      "\n",
      "Test set: Accuracy: 424.0/640 (66%)\n",
      "\n",
      "Train Epoch: 5 [0/640 (0%)]\tLoss: 0.525000\tTime: 4.026s\n",
      "Train Epoch: 5 [64/640 (10%)]\tLoss: 0.435000\tTime: 4.066s\n",
      "Train Epoch: 5 [128/640 (20%)]\tLoss: 0.559000\tTime: 4.073s\n",
      "Train Epoch: 5 [192/640 (30%)]\tLoss: 0.459000\tTime: 4.088s\n",
      "Train Epoch: 5 [256/640 (40%)]\tLoss: 0.454000\tTime: 4.103s\n",
      "Train Epoch: 5 [320/640 (50%)]\tLoss: 0.451000\tTime: 4.087s\n",
      "Train Epoch: 5 [384/640 (60%)]\tLoss: 0.480000\tTime: 4.112s\n",
      "Train Epoch: 5 [448/640 (70%)]\tLoss: 0.510000\tTime: 4.130s\n",
      "Train Epoch: 5 [512/640 (80%)]\tLoss: 0.501000\tTime: 4.170s\n",
      "Train Epoch: 5 [576/640 (90%)]\tLoss: 0.567000\tTime: 4.097s\n",
      "\n",
      "Test set: Accuracy: 449.0/640 (70%)\n",
      "\n",
      "Train Epoch: 6 [0/640 (0%)]\tLoss: 0.476000\tTime: 4.153s\n",
      "Train Epoch: 6 [64/640 (10%)]\tLoss: 0.387000\tTime: 4.176s\n",
      "Train Epoch: 6 [128/640 (20%)]\tLoss: 0.516000\tTime: 4.239s\n",
      "Train Epoch: 6 [192/640 (30%)]\tLoss: 0.410000\tTime: 4.286s\n",
      "Train Epoch: 6 [256/640 (40%)]\tLoss: 0.412000\tTime: 4.359s\n",
      "Train Epoch: 6 [320/640 (50%)]\tLoss: 0.406000\tTime: 4.303s\n",
      "Train Epoch: 6 [384/640 (60%)]\tLoss: 0.438000\tTime: 4.292s\n",
      "Train Epoch: 6 [448/640 (70%)]\tLoss: 0.471000\tTime: 4.288s\n",
      "Train Epoch: 6 [512/640 (80%)]\tLoss: 0.462000\tTime: 4.347s\n",
      "Train Epoch: 6 [576/640 (90%)]\tLoss: 0.529000\tTime: 4.359s\n",
      "\n",
      "Test set: Accuracy: 464.0/640 (72%)\n",
      "\n",
      "Train Epoch: 7 [0/640 (0%)]\tLoss: 0.434000\tTime: 4.554s\n",
      "Train Epoch: 7 [64/640 (10%)]\tLoss: 0.352000\tTime: 4.660s\n",
      "Train Epoch: 7 [128/640 (20%)]\tLoss: 0.476000\tTime: 4.629s\n",
      "Train Epoch: 7 [192/640 (30%)]\tLoss: 0.378000\tTime: 4.719s\n",
      "Train Epoch: 7 [256/640 (40%)]\tLoss: 0.375000\tTime: 4.860s\n",
      "Train Epoch: 7 [320/640 (50%)]\tLoss: 0.368000\tTime: 4.745s\n",
      "Train Epoch: 7 [384/640 (60%)]\tLoss: 0.403000\tTime: 4.513s\n",
      "Train Epoch: 7 [448/640 (70%)]\tLoss: 0.440000\tTime: 4.559s\n",
      "Train Epoch: 7 [512/640 (80%)]\tLoss: 0.428000\tTime: 4.649s\n",
      "Train Epoch: 7 [576/640 (90%)]\tLoss: 0.499000\tTime: 4.660s\n",
      "\n",
      "Test set: Accuracy: 469.0/640 (73%)\n",
      "\n",
      "Train Epoch: 8 [0/640 (0%)]\tLoss: 0.407000\tTime: 4.558s\n",
      "Train Epoch: 8 [64/640 (10%)]\tLoss: 0.323000\tTime: 4.625s\n",
      "Train Epoch: 8 [128/640 (20%)]\tLoss: 0.447000\tTime: 4.692s\n",
      "Train Epoch: 8 [192/640 (30%)]\tLoss: 0.349000\tTime: 5.024s\n",
      "Train Epoch: 8 [256/640 (40%)]\tLoss: 0.348000\tTime: 4.977s\n",
      "Train Epoch: 8 [320/640 (50%)]\tLoss: 0.342000\tTime: 4.871s\n",
      "Train Epoch: 8 [384/640 (60%)]\tLoss: 0.375000\tTime: 4.719s\n",
      "Train Epoch: 8 [448/640 (70%)]\tLoss: 0.411000\tTime: 4.706s\n",
      "Train Epoch: 8 [512/640 (80%)]\tLoss: 0.403000\tTime: 4.809s\n",
      "Train Epoch: 8 [576/640 (90%)]\tLoss: 0.475000\tTime: 4.738s\n",
      "\n",
      "Test set: Accuracy: 474.0/640 (74%)\n",
      "\n",
      "Train Epoch: 9 [0/640 (0%)]\tLoss: 0.384000\tTime: 4.954s\n",
      "Train Epoch: 9 [64/640 (10%)]\tLoss: 0.301000\tTime: 5.081s\n",
      "Train Epoch: 9 [128/640 (20%)]\tLoss: 0.421000\tTime: 5.052s\n",
      "Train Epoch: 9 [192/640 (30%)]\tLoss: 0.327000\tTime: 5.100s\n",
      "Train Epoch: 9 [256/640 (40%)]\tLoss: 0.325000\tTime: 5.167s\n",
      "Train Epoch: 9 [320/640 (50%)]\tLoss: 0.318000\tTime: 5.194s\n",
      "Train Epoch: 9 [384/640 (60%)]\tLoss: 0.353000\tTime: 5.207s\n",
      "Train Epoch: 9 [448/640 (70%)]\tLoss: 0.391000\tTime: 5.322s\n",
      "Train Epoch: 9 [512/640 (80%)]\tLoss: 0.379000\tTime: 5.285s\n",
      "Train Epoch: 9 [576/640 (90%)]\tLoss: 0.455000\tTime: 5.268s\n",
      "\n",
      "Test set: Accuracy: 481.0/640 (75%)\n",
      "\n",
      "Train Epoch: 10 [0/640 (0%)]\tLoss: 0.363000\tTime: 5.530s\n",
      "Train Epoch: 10 [64/640 (10%)]\tLoss: 0.281000\tTime: 5.555s\n",
      "Train Epoch: 10 [128/640 (20%)]\tLoss: 0.398000\tTime: 6.025s\n",
      "Train Epoch: 10 [192/640 (30%)]\tLoss: 0.306000\tTime: 5.286s\n",
      "Train Epoch: 10 [256/640 (40%)]\tLoss: 0.306000\tTime: 5.300s\n",
      "Train Epoch: 10 [320/640 (50%)]\tLoss: 0.296000\tTime: 5.410s\n",
      "Train Epoch: 10 [384/640 (60%)]\tLoss: 0.332000\tTime: 5.457s\n",
      "Train Epoch: 10 [448/640 (70%)]\tLoss: 0.371000\tTime: 5.527s\n",
      "Train Epoch: 10 [512/640 (80%)]\tLoss: 0.356000\tTime: 5.646s\n",
      "Train Epoch: 10 [576/640 (90%)]\tLoss: 0.435000\tTime: 5.506s\n",
      "\n",
      "Test set: Accuracy: 488.0/640 (76%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision() \n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "source": [
    "## 6 ç›¸å…³è®¨è®º\n",
    "\n",
    "## 6.1è®¡ç®—æ—¶é—´\n",
    "\n",
    "ç¬¬ä¸€ä»¶äº‹æ˜¾ç„¶æ˜¯è¿è¡Œæ—¶é—´ï¼æ‚¨è‚¯å®šå·²ç»æ³¨æ„åˆ°ï¼Œå®ƒæ¯”çº¯æ–‡æœ¬è®­ç»ƒè¦æ…¢å¾—å¤šã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨1æ‰¹64é¡¹ä¸Šè¿›è¡Œä¸€æ¬¡è¿­ä»£éœ€è¦3.2 sï¼Œè€Œåœ¨çº¯PyTorchä¸­åªæœ‰13 msã€‚å°½ç®¡è¿™ä¼¼ä¹Žæ˜¯ä¸€ä¸ªé˜»æ­¢ç¨‹åºï¼Œä½†è¯·å›žæƒ³ä¸€ä¸‹ï¼Œè¿™é‡Œçš„æ‰€æœ‰äº‹æƒ…éƒ½æ˜¯è¿œç¨‹å‘ç”Ÿçš„ï¼Œå¹¶ä¸”æ˜¯åœ¨åŠ å¯†çš„ä¸–ç•Œä¸­å‘ç”Ÿçš„ï¼šæ²¡æœ‰å•ä¸ªæ•°æ®é¡¹è¢«å…¬å¼€ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œå¤„ç†ä¸€é¡¹çš„æ—¶é—´ä¸º50msï¼Œè¿™è¿˜ä¸é”™ã€‚çœŸæ­£çš„é—®é¢˜æ˜¯åˆ†æžä½•æ—¶éœ€è¦åŠ å¯†è®­ç»ƒä»¥åŠä½•æ—¶ä»…åŠ å¯†é¢„æµ‹å°±è¶³å¤Ÿäº†ã€‚ä¾‹å¦‚ï¼Œåœ¨ç”Ÿäº§å°±ç»ªçš„æƒ…å†µä¸‹ï¼Œå®Œå…¨å¯ä»¥æŽ¥å—50æ¯«ç§’æ‰§è¡Œé¢„æµ‹ï¼\n",
    "\n",
    "ä¸€ä¸ªä¸»è¦çš„ç“¶é¢ˆæ˜¯æ˜‚è´µçš„æ¿€æ´»åŠŸèƒ½çš„ä½¿ç”¨ï¼šSMPCçš„reluæ¿€æ´»éžå¸¸æ˜‚è´µï¼Œå› ä¸ºå®ƒä½¿ç”¨ç§æœ‰æ¯”è¾ƒå’ŒSecureNNåè®®ã€‚ä¸¾ä¾‹è¯´æ˜Žï¼Œå¦‚æžœæˆ‘ä»¬ç”¨äºŒæ¬¡æ¿€æ´»ä»£æ›¿reluï¼Œå°±åƒåœ¨CryptoNetsç­‰åŠ å¯†è®¡ç®—çš„å‡ ç¯‡è®ºæ–‡ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†ä»Ž3.2sé™åˆ°1.2sã€‚\n",
    "\n",
    "é€šå¸¸ï¼Œè¦åˆ é™¤çš„å…³é”®æ€æƒ³æ˜¯ä»…åŠ å¯†å¿…è¦çš„å†…å®¹ï¼Œæœ¬æ•™ç¨‹å‘æ‚¨å±•ç¤ºäº†å®ƒçš„ç®€å•æ€§\n",
    "\n",
    "## 6.2ä½¿ç”¨SMPCè¿›è¡Œåå‘ä¼ æ’­\n",
    "æ‚¨å¯èƒ½æƒ³çŸ¥é“æˆ‘ä»¬å¦‚ä½•æ‰§è¡Œåå‘ä¼ æ’­å’Œæ¢¯åº¦æ›´æ–°ï¼Œå°½ç®¡æˆ‘ä»¬æ­£åœ¨æœ‰é™åŸŸä¸­ä½¿ç”¨æ•´æ•°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ–°çš„syftå¼ é‡ï¼Œç§°ä¸ºAutogradTensorã€‚å°½ç®¡æ‚¨å¯èƒ½è¿˜æ²¡æœ‰çœ‹è¿‡æœ¬æ•™ç¨‹ï¼Œä½†å®ƒè¿˜æ˜¯å¤§é‡ä½¿ç”¨å®ƒï¼è®©æˆ‘ä»¬é€šè¿‡æ‰“å°æ¨¡åž‹çš„é‡é‡è¿›è¡Œæ£€æŸ¥ï¼š\n",
    "\n",
    "## 6.3å®‰å…¨ä¿éšœ\n",
    "æœ€åŽï¼Œè®©æˆ‘ä»¬ç»™å‡ºä¸€äº›æœ‰å…³æˆ‘ä»¬åœ¨æ­¤å¤„å®žçŽ°çš„å®‰å…¨æ€§çš„æç¤ºï¼šæˆ‘ä»¬åœ¨è¿™é‡Œè€ƒè™‘çš„å¯¹æ‰‹æ˜¯è¯šå®žä½†å¥½å¥‡çš„ï¼šè¿™æ„å‘³ç€å¯¹æ‰‹æ— æ³•é€šè¿‡è¿è¡Œæ­¤åè®®æ¥å­¦ä¹ æœ‰å…³æ•°æ®çš„ä»»ä½•ä¿¡æ¯ï¼Œä½†æ˜¯æ¶æ„çš„å¯¹æ‰‹å¯ä»¥ä»ç„¶åç¦»åè®®ï¼Œä¾‹å¦‚å°è¯•ç ´åå…±äº«ä»¥ç ´åè®¡ç®—ã€‚åœ¨æ­¤ç±»SMPCè®¡ç®—ï¼ˆåŒ…æ‹¬ç§æœ‰æ¯”è¾ƒï¼‰ä¸­é’ˆå¯¹æ¶æ„å¯¹æ‰‹çš„å®‰å…¨æ€§ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œå³ä½¿â€œå®‰å…¨å¤šæ–¹è®¡ç®—â€ç¡®ä¿ä¸è®¿é—®åŸ¹è®­æ•°æ®ï¼Œæ­¤å¤„ä»ç„¶å­˜åœ¨æ¥è‡ªçº¯æ–‡æœ¬ä¸–ç•Œçš„è®¸å¤šå¨èƒã€‚ä¾‹å¦‚ï¼Œå½“æ‚¨å¯ä»¥å‘æ¨¡åž‹æå‡ºè¯·æ±‚æ—¶ï¼ˆåœ¨MLaaSçš„ä¸Šä¸‹æ–‡ä¸­ï¼‰ï¼Œæ‚¨å¯ä»¥èŽ·å¾—å¯èƒ½æ³„éœ²æœ‰å…³è®­ç»ƒæ•°æ®é›†ä¿¡æ¯çš„é¢„æµ‹ã€‚ç‰¹åˆ«æ˜¯ï¼Œæ‚¨æ²¡æœ‰é’ˆå¯¹æˆå‘˜èµ„æ ¼æ”»å‡»çš„ä»»ä½•ä¿æŠ¤æŽªæ–½ï¼Œè¿™æ˜¯å¯¹æœºå™¨å­¦ä¹ æœåŠ¡çš„å¸¸è§æ”»å‡»ï¼Œåœ¨è¿™ç§æ”»å‡»ä¸­ï¼Œå¯¹æ‰‹è¦ç¡®å®šæ˜¯å¦åœ¨æ•°æ®é›†ä¸­ä½¿ç”¨äº†ç‰¹å®šé¡¹ç›®ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå…¶ä»–æ”»å‡»ï¼Œä¾‹å¦‚æ„å¤–çš„è®°å¿†è¿‡ç¨‹ï¼ˆæ¨¡åž‹å­¦ä¹ æœ‰å…³æ•°æ®é¡¹çš„ç‰¹å®šç‰¹å¾çš„æ¨¡åž‹ï¼‰ï¼Œæ¨¡åž‹å€’ç½®æˆ–æå–ï¼Œä»ç„¶æ˜¯å¯èƒ½çš„ã€‚\n",
    "\n",
    "å¯¹ä¸Šè¿°è®¸å¤šå¨èƒæœ‰æ•ˆçš„ä¸€ç§é€šç”¨è§£å†³æ–¹æ¡ˆæ˜¯æ·»åŠ å·®å¼‚éšç§ã€‚å®ƒå¯ä»¥ä¸Žå®‰å…¨çš„å¤šæ–¹è®¡ç®—å®Œç¾Žåœ°ç»“åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä¸”å¯ä»¥æä¾›éžå¸¸æœ‰è¶£çš„å®‰å…¨æ€§ä¿è¯ã€‚æˆ‘ä»¬ç›®å‰æ­£åœ¨ç ”ç©¶å‡ ç§å®žçŽ°æ–¹å¼ï¼Œå¹¶å¸Œæœ›æå‡ºä¸€ä¸ªå°†ä¸¤è€…ç»“åˆèµ·æ¥çš„ç¤ºä¾‹ï¼"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}